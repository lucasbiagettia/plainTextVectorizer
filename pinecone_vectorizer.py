# -*- coding: utf-8 -*-
"""pinecone_vectorizer

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13UjeyVsdGLilcB4PVib0NbSwti-hV20R

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb)

# RAG with LLaMa 13B

In this notebook we'll explore how we can use the open source **Llama-13b-chat** model in both Hugging Face transformers and LangChain.
At the time of writing, you must first request access to Llama 2 models via [this form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) (access is typically granted within a few hours). If you need guidance on getting access please refer to the beginning of this [article](https://www.pinecone.io/learn/llama-2/) or [video](https://youtu.be/6iHVJyX2e50?t=175).

---

üö® _Note that running this on CPU is sloooow. If running on Google Colab you can avoid this by going to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**. This should be included within the free tier of Colab._

---

We start by doing a `pip install` of all required libraries.
"""

!pip install -qU \
  transformers==4.31.0 \
  sentence-transformers==2.2.2 \
  pinecone-client==2.2.2 \
  datasets==2.14.0 \
  accelerate==0.21.0 \
  einops==0.6.1 \
  langchain==0.0.240 \
  xformers==0.0.20 \
  bitsandbytes==0.41.0

"""## Initializing the Hugging Face Embedding Pipeline

We begin by initializing the embedding pipeline that will handle the transformation of our docs into vector embeddings. We will use the `sentence-transformers/all-MiniLM-L6-v2` model for embedding.
"""

# Abre el archivo en modo lectura, especificando la codificaci√≥n UTF-8
with open('/content/manifiesto comunista.txt', 'r', encoding='utf-8') as file:
    # Lee el contenido del archivo y almac√©nalo en una variable string
    contenido = file.read()

from torch import cuda
from langchain.embeddings.huggingface import HuggingFaceEmbeddings

embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'

device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'

embed_model = HuggingFaceEmbeddings(
    model_name=embed_model_id,
    model_kwargs={'device': device},
    encode_kwargs={'device': device, 'batch_size': 32}
)

from torch import cuda
from langchain.embeddings.huggingface import HuggingFaceEmbeddings

docs = [contenido]

embeddings = embed_model.embed_documents(docs)

"""We can use the embedding model to create document embeddings like so:"""



"""## Building the Vector Index

We now need to use the embedding pipeline to build our embeddings and store them in a Pinecone vector index. To begin we'll initialize our index, for this we'll need a [free Pinecone API key](https://app.pinecone.io/).
"""

import os
import pinecone

# get API key from app.pinecone.io and environment from console
pinecone.init(
    api_key='571819e1-2c00-4ab3-a7b0-8690c0d0ae1a',
    environment='gcp-starter'
)

"""Now we initialize the index."""

import time

index_name = 'llama-2-rag'

if index_name not in pinecone.list_indexes():
    pinecone.create_index(
        index_name,
        dimension=len(embeddings[0]),
        metric='cosine'
    )
    # wait for index to finish initialization
    while not pinecone.describe_index(index_name).status['ready']:
        time.sleep(1)

"""Now we connect to the index:"""

index = pinecone.Index(index_name)
index.describe_index_stats()

"""With our index and embedding process ready we can move onto the indexing process itself. For that, we'll need a dataset. We will use a set of Arxiv papers related to (and including) the Llama 2 research paper."""

# Importar las bibliotecas necesarias
import pandas as pd

# Leer el texto plano
texto_plano = contenido

# Dividir el texto plano en chunks
chunks = texto_plano.split("\n\n")

# Etiquetar los chunks
etiquetas = ["abstract"] * len(chunks)
etiquetas[0] = "introducci√≥n"
etiquetas[-1] = "conclusi√≥n"

# Guardar el dataset
df = pd.DataFrame({"chunk": chunks, "etiqueta": etiquetas})
data = df
df.head()

"""We will embed and index the documents like so:"""

#data = data.to_pandas()

batch_size = 32


for i in range(0, len(data), batch_size):
    i_end = min(len(data), i+batch_size)
    batch = data.iloc[i:i_end]

    # Create unique identifiers
    ids = [f"{x['chunk']}-{x['etiqueta']}" for i, x in batch.iterrows()]

    # Extract text content
    texts = [x['chunk'] for i, x in batch.iterrows()]

    # Generate vector embeddings
    embeds = embed_model.embed_documents(texts)

    # Prepare metadata
    metadata = [
        {'text': x['chunk'],
         'title': x['etiqueta']} for i, x in batch.iterrows()
    ]

index.describe_index_stats()

"""## Initializing the Hugging Face Pipeline

The first thing we need to do is initialize a `text-generation` pipeline with Hugging Face transformers. The Pipeline requires three things that we must initialize first, those are:

* A LLM, in this case it will be `meta-llama/Llama-2-13b-chat-hf`.

* The respective tokenizer for the model.

We'll explain these as we get to them, let's begin with our model.

We initialize the model and move it to our CUDA-enabled GPU. Using Colab this can take 5-10 minutes to download and initialize the model.
"""

from torch import cuda, bfloat16
import transformers

model_id = 'meta-llama/Llama-2-7b-chat-hf'

device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'

# set quantization configuration to load large model with less GPU memory
# this requires the `bitsandbytes` library
bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=bfloat16
)

# begin initializing HF items, need auth token for these
hf_auth = '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'
model_config = transformers.AutoConfig.from_pretrained(
    model_id,
    use_auth_token=hf_auth
)

model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    quantization_config=bnb_config,
    device_map='auto',
    use_auth_token=hf_auth
)
model.eval()
print(f"Model loaded on {device}")

"""The pipeline requires a tokenizer which handles the translation of human readable plaintext to LLM readable token IDs. The Llama 2 13B models were trained using the Llama 2 13B tokenizer, which we initialize like so:"""

tokenizer = transformers.AutoTokenizer.from_pretrained(
    model_id,
    use_auth_token=hf_auth
)

"""Now we're ready to initialize the HF pipeline. There are a few additional parameters that we must define here. Comments explaining these have been included in the code."""

generate_text = transformers.pipeline(
    model=model, tokenizer=tokenizer,
    return_full_text=True,  # langchain expects the full text
    task='text-generation',
    # we pass model parameters here too
    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max
    max_new_tokens=512,  # mex number of tokens to generate in the output
    repetition_penalty=1.1  # without this output begins repeating
)

"""Confirm this is working:

Now to implement this in LangChain
"""

from langchain.llms import HuggingFacePipeline

llm = HuggingFacePipeline(pipeline=generate_text)

"""We still get the same output as we're not really doing anything differently here, but we have now added **Llama 2 13B Chat** to the LangChain library. Using this we can now begin using LangChain's advanced agent tooling, chains, etc, with **Llama 2**.

## Initializing a RetrievalQA Chain

For **R**etrieval **A**ugmented **G**eneration (RAG) in LangChain we need to initialize either a `RetrievalQA` or `RetrievalQAWithSourcesChain` object. For both of these we need an `llm` (which we have initialized) and a Pinecone index ‚Äî but initialized within a LangChain vector store object.

Let's begin by initializing the LangChain vector store, we do it like so:
"""

from langchain.vectorstores import Pinecone

text_field = 'text'  # field in metadata that contains text content

vectorstore = Pinecone(
    index, embed_model.embed_query, text_field
)

"""We can confirm this works like so:

Looks good! Now we can put our `vectorstore` and `llm` together to create our RAG pipeline.
"""

from langchain.chains import RetrievalQA

rag_pipeline = RetrievalQA.from_chain_type(
    llm=llm, chain_type='stuff',
    retriever=vectorstore.as_retriever()
)

"""Let's begin asking questions! First let's try *without* RAG:"""

llm('Explicame que es el proletariado')

"""Hmm, that's not what we meant... What if we use our RAG pipeline?"""

rag_pipeline('Explicame que es el proletariado')

"""[texto del v√≠nculo](https://)This looks *much* better! Let's try some more.

'result': ' In the Manifesto of the Communist Party, Karl Marx describes how capitalism divides society into two main classes: the bourgeoisie and the proletariat. The bourgeoisie refers to the modern capitalists who own the means of production and employ wage laborers. On the other hand, the proletariat consists of modern wage laborers who lack the means of production and must sell their labor to survive. This division leads to class antagonisms and conflict, driving historical change and revolution.'}
"""